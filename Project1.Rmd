---
title: "SCS project 1"
output: html_notebook
---

```{r}
library(caret)

#change this to whatever your own directory is 
source("~/Desktop/SCSProj1/stylometryfunctions.R")

# as above
M <- loadCorpus("~/Desktop/SCSProj1/FunctionWords/", "frequentwords70")

set.seed(12345L)
```

### Code Used to Produce Results
Code chunks used in our analysis of Frankenstein.
Includes: visualisations of the data, a sample training and test set applied to both methods, cross-validation of both methods, confusion matrices for our methods, and finally applying the models to Frankenstein to see how it performs. 


```{r} 
#Visualisaiton of the data with mds

x <- NULL
for (i in 1:length(M$features)){
  x <- rbind(x, apply(M$features[[i]], 2, sum))
}

for (i in 1:nrow(x)){
  x[i,] <- x[i,] / sum(x[i,])
}
for (j in 1:ncol(x)) {
  x[,j] <- (x[,j] - mean(x[,j]))/sd(x[,j])
}

d <- dist(x)
pts <- cmdscale(d)
plot(pts,type='n')
text(pts[,1],pts[,2],label=M$authornames,cex=0.8)


```
```{r}
x <- NULL           
doc_authors <- c() 

# Loop over each author
for (i in 1:length(M$features)) {
  current_data <- M$features[[i]]  
  
  # N docs for current author i 
  num_docs <- nrow(current_data)
  
  # add document data to x
  x <- rbind(x, current_data)
  
  # document authors
  doc_authors <- c(doc_authors, rep(M$authornames[i], num_docs))
}

# normalise/standardise data in books
for (i in 1:nrow(x)) {
  x[i, ] <- x[i, ] / sum(x[i, ])
}

for (j in 1:ncol(x)) {
  x[, j] <- (x[, j] - mean(x[, j])) / sd(x[, j])
}

# MDS co-ords
dist_matrix <- dist(x)
pts <- cmdscale(dist_matrix)

# indicator for unknown author
IsAuthor9 <- doc_authors == M$authornames[9]  

## ggplot dataframe
plot_data <- data.frame(
  Dim1 = pts[, 1],
  Dim2 = pts[, 2],
  Author = doc_authors,
  IsAuthor9 = IsAuthor9
)

ggplot(plot_data, aes(x = Dim1, y = Dim2)) +
  geom_point(aes(color = Author, shape = IsAuthor9), size = 2) +
  scale_shape_manual(
    values = c('FALSE' = 16, 'TRUE' = 4),  # 16: circle, 4: 'x'
    labels = c('FALSE' = 'Known Author', 'TRUE' = 'Unknown')
  ) +
  theme_classic() +
  theme(legend.position = "right",
        legend.text = element_text(size=4)) +
  guides(
    shape = guide_legend(title = "Document Type"),
    color = guide_legend(override.aes = list(size = 2))
  ) +
  labs(
    title = "MDS Plot of Books",
    x = "Dimension 1",
    y = "Dimension 2")




```
```{r}
# Visualization of the data with MDS second time

x <- NULL           
doc_authors <- c() 

# Loop over each author
for (i in 1:length(M$features)) {
  current_data <- M$features[[i]]  
  
  # N docs for current author i 
  num_docs <- nrow(current_data)
  
  # add document data to x
  x <- rbind(x, current_data)
  
  # document authors
  doc_authors <- c(doc_authors, rep(M$authornames[i], num_docs))
}

# normalise/standardise data in books
for (i in 1:nrow(x)) {
  x[i, ] <- x[i, ] / sum(x[i, ])
}

for (j in 1:ncol(x)) {
  x[, j] <- (x[, j] - mean(x[, j])) / sd(x[, j])
}

# MDS co-ords
dist_matrix <- dist(x)
pts <- cmdscale(dist_matrix)

# indicator for unknown author
IsAuthor9 <- doc_authors == M$authornames[9]  

## ggplot dataframe
plot_data <- data.frame(
  Dim1 = pts[, 1],
  Dim2 = pts[, 2],
  Author = doc_authors,
  IsAuthor9 = IsAuthor9
)

ggplot(plot_data, aes(x = Dim2, y = Dim1)) +
  geom_point(aes(color = Author, shape = IsAuthor9), size = 2) +
  scale_shape_manual(
    values = c('FALSE' = 16, 'TRUE' = 4),  # 16: circle, 4: 'x'
    labels = c('FALSE' = 'Known Author', 'TRUE' = 'Unknown')
  ) +
  theme_classic() +
  theme(legend.position = "right",
        legend.text = element_text(size=4)) +
  guides(
    shape = guide_legend(title = "Document Type"),
    color = guide_legend(override.aes = list(size = 2))
  ) +
  labs(x = "Dimension 2",
       y = "Dimension 1")


```
## rough comments about the visualisation

The visualisation indicates that there is significant overlap between many of the authors, particularly Mary Shelley. Notable outliers to this trend include Bram Stoker, an irish novelist who was active much later than the others, Percy Shelley's poetry works which may be due to the relationship previously discussed between word usage differences between prose and poetry, and Charles Brockden Brown who was an american novelist. A difference in nationality and 'accent' might be the root cause of this. The other authors were all active during a similar time period and from the same country. This overlap may cause issues with our classification models, particularly DA as this relies on estimating word frequencies used by authors, and as we can see there appears to be a great deal of variability in this given the apparent lack of distinction between many of the authors. 

Nevertheless, there is enough differentiation between them to be able to be reasonably confident in the methods we implement, although Mary Shelley may cause issues due to her being fairly 'middle of the pack'- her works look to be the most varied stylistically as well as appearing in and around many other authors. 


```{r}
# discriminant analysis and K-nearest neighbours
# taking a sample to see whats happening with a sample training and test set.
## not included in the report ##

traindata <- M$features
testdata <- NULL
testlabels <- NULL

for( i in 1:length(traindata)){
  
  # keep author in training set if they have only the one book
  if (nrow(M$features[[i]]) <= 1){  
    next
  }
  
  #select a random book by this author by choosing a row (= book)
    testind <- sample(1:nrow(traindata[[i]]), 1)

    #add this book to the test set

    testdata <- rbind(testdata, traindata[[i]][testind,])

    testlabels <- c(testlabels, i)

    #now discard the book from the training set

    traindata[[i]] <- traindata[[i]][-testind,,drop=FALSE]
    
}

# obtaining DA prediction and accuracy (naive)
DApreds <- discriminantCorpus(traindata, testdata)
DAacc <- sum(DApreds == testlabels) / length(testlabels)

# obtaining KNN prediction and accuracy (naive)
KNNpreds <- KNNCorpus(traindata, testdata)
KNNacc <- sum(KNNpreds == testlabels) / length(testlabels)
DAacc
KNNacc



```
---

---
```{r}
# LOOCV code

DApredictions <- NULL
KNNpredictions <- NULL
truth <- NULL
features <- M$features

# Filter out authors with only one document, deliberate choice
# to balance out bias as opposed to having 100% or 0% accuracy on these authors
authors_to_keep <- sapply(features, function(x) nrow(x) > 1)
features_filtered <- features[authors_to_keep]

for (i in 1:length(features_filtered)) {
  for (j in 1:nrow(features_filtered[[i]])) {
    testdata <- matrix(features_filtered[[i]][j, ], nrow = 1)
    traindata <- features_filtered
    traindata[[i]] <- traindata[[i]][-j, , drop = FALSE]
    
    pred <- discriminantCorpus(traindata, testdata)
    DApredictions <- c(DApredictions, pred)
    pred <- KNNCorpus(traindata, testdata)
    KNNpredictions <- c(KNNpredictions, pred)
    truth <- c(truth, i)
    
  }
}

LOOCVDAacc<- sum(DApredictions == truth) / length(truth)
LOOCVKNNacc <- sum(KNNpredictions == truth) / length(truth)

print("LOOCV accuracy for DA:")
LOOCVDAacc
print("LOOCV accuracy for KNN:")
LOOCVKNNacc



```
Our LOOCV tests indicate that whilst both do have an acceptable level of accuracy, we should trust KNN more since it has a significantly smaller error rate (=1 - accuracy) than DA. 


```{r}
library(caret)

DAconfusionmatrix <- confusionMatrix(as.factor(DApredictions), 
                                     as.factor(truth))

KNNconfusionmatrix <- confusionMatrix(as.factor(KNNpredictions),
                                      as.factor(truth))
print("DA cm:")
DAconfusionmatrix
print("---------------------------------------------")
print("KNN confusion matrix")
KNNconfusionmatrix
```

```{r}
# making more tables than ikea for the overleaf document

library(knitr)
# table for point estimate accuracies
accuracies <- c(DAacc, KNNacc)
data1 <- matrix(accuracies, ncol=1, nrow=2)
colnames(data1) = c("Accuracy")
rownames(data1) = c("DA", 'KNN')


table1 <- kable(data1,
                format = "latex",
                booktabs = TRUE,
                caption = "Point Accuracies of Stylometry Methods",
                linesep = "")

LOOCVaccuracies <- c(LOOCVDAacc, LOOCVKNNacc)
data2 <- matrix (LOOCVaccuracies, ncol=1, nrow=2)
colnames(data2) = c("Accuracy")
rownames(data2) = c("DA","KNN")

table2 <- kable(data2, 
                format = "latex",
                booktabs = TRUE,
                caption = "LOOCV Accuracies of Stylometry Methods",
                linesep = "")

# table for frankenstein analysis

prediction_frankenstein <- c(new_authors[frankenstein_da],
                             new_authors[frankenstein_knn])
data3 <- matrix(prediction_frankenstein, ncol=2)
colnames(data3) = c("DA Prediction", "KNN Prediction")

table3 <- kable(data3, 
                format = "latex",
                booktabs = T,
                caption = "Frankenstein Authorship Prediction, by Method",
                linesep = "")


```



## Analysis of Frankenstein



```{r}
# finding out who wrote frankenstein, using everything apart from 
# frankenstein in the training set and just that novel in the test set.

trainset <- M$features[-9]
testset <- M$features[[9]]

new_authors <- M$authornames[-9]

frankenstein_da <- discriminantCorpus(trainset, testset)
frankenstein_knn <- KNNCorpus(trainset, testset)

print("DA prediction of author:")
new_authors[frankenstein_da]
print("KNN prediction of author:")
new_authors[frankenstein_knn]
```
 
