---
title: "SCS project1 draft"
output: html_notebook
---

```{r}
library(caret)

#change this to whatever your own directory is 
source("~/Desktop/SCSProj1/stylometryfunctions.R")

# as above
M <- loadCorpus("~/Desktop/SCSProj1/FunctionWords/", "frequentwords70")

set.seed(1)
```

### The Dataset

The data provided consisted of 'features' which are the word counts for the 70 function words used by each author. These are vectors of 70 elements for each individual book written by the authors. There is also author names which is used to connect the books with the known author of them. 

Authors included in teh data set include Mary Shelley, Percy Shelley, Sir Walter Scott, William Godwin, Bram Stoker, as well as others. Since Percy Shelley was primarily a poet as opposed to a novelist, a selection of his poetry has been included alongside his prose to give a greater quantity of data to draw upon. We have, however, classified Percy Shelley's works into both a poetry and prose authorship in order to better be able to group categorise his work for the reason that the frequency in which one would use function words in both poetry and prose may be quite different. 

There is also works included that are known to be co-authored by Mary and Percy Shelley. Again, these are categorised as joint-authorship and not included in the corpuses of Mary or Percy exclusively. 



```{r}
# discriminant analysis and K-nearest neighbours

traindata <- M$features
testdata <- NULL
testlabels <- NULL

for( i in 1:length(traindata)){
  
  # keep author in training set if they have only the one book
  if (nrow(M$features[[i]]) <= 1){  
    next
  }
  
  #select a random book by this author by choosing a row (= book)
    testind <- sample(1:nrow(traindata[[i]]), 1)

    #add this book to the test set

    testdata <- rbind(testdata, traindata[[i]][testind,])

    testlabels <- c(testlabels, i)

    #now discard the book from the training set

    traindata[[i]] <- traindata[[i]][-testind,,drop=FALSE]
    
}

# obtaining DA prediction and accuracy (naive)
DApreds <- discriminantCorpus(traindata, testdata)
DAacc <- sum(DApreds == testlabels) / length(testlabels)

# obtaining KNN prediction and accuracy (naive)
KNNpreds <- KNNCorpus(traindata, testdata)
KNNacc <- sum(KNNpreds == testlabels) / length(testlabels)


```
---
---
```{r}


DApredictions <- NULL
KNNpredictions <- NULL
truth <- NULL
features <- M$features

# Filter out authors with only one document
authors_to_keep <- sapply(features, function(x) nrow(x) > 1)
features_filtered <- features[authors_to_keep]

for (i in 1:length(features_filtered)) {
  for (j in 1:nrow(features_filtered[[i]])) {
    testdata <- matrix(features_filtered[[i]][j, ], nrow = 1)
    traindata <- features_filtered
    traindata[[i]] <- traindata[[i]][-j, , drop = FALSE]
    
    pred <- discriminantCorpus(traindata, testdata)
    DApredictions <- c(DApredictions, pred)
    pred <- KNNCorpus(traindata, testdata)
    KNNpredictions <- c(KNNpredictions, pred)
    truth <- c(truth, i)
  }
}

DAacc<- sum(DApredictions == truth) / length(truth)
KNNacc <- sum(KNNpredictions == truth) / length(truth)
print("LOOCV accuracy for DA:")
DAacc
print("LOOCV accuracy for KNN:")
KNNacc


```
---


```{r}
library(caret)

DAconfusionmatrix <- confusionMatrix(as.factor(DApredictions), 
                                     as.factor(truth))

KNNconfusionmatrix <- confusionMatrix(as.factor(KNNpredictions),
                                      as.factor(truth))
print("DA cm:")
DAconfusionmatrix
print("---------------------------------------------")
print("KNN confusion matrix")
KNNconfusionmatrix
```

## Analysis of Frankenstein

We shall now apply our methods of Stylometry in an attempt to identify the author of Frankenstein. We shall use the entire corpus obtained as the training set, and test the model in both a KNN and DA setting on the Frankenstein novel. Each model choice will make a prediction as to whom the author is. 

```{r}
# finding out who wrote frankenstein, using everything apart from 
# frankenstein in the training set and just that novel in the test set.

trainset <- M$features[-9]
testset <- M$features[[9]]

new_authors <- M$authornames[-9]

frankenstein_da <- discriminantCorpus(trainset, testset)
frankenstein_knn <- KNNCorpus(trainset, testset)

print("DA prediction of author:")
new_authors[frankenstein_da]
print("KNN prediction of author:")
new_authors[frankenstein_knn]
```
Here we observe that there is disagreement between the models, with discriminant analysis predicting that William Godwin was the true author of Frankenstein, whilst the K-nearest-neighbours methodology predicts that Mary Shelley is the author of the novel. However from our cross-validation we have observed that KNN is far superior for identifying authorship on this dataset than discriminant analysis is. For example, viewing the confusion matrix above indicated that many books using discriminant analysis were misidentified as being the work of Mary Shelley, including one of Godwin's own books. K-nearest neighbours does not, however, have the same issues, demonstrating a high degree of accuracy when tested using cross-validation methods.

There may also be a degree of uncertainty regarding the DA classification of teh athor of Frankenstein. This may be for reasons such as Frankenstein being 

Therefore, this leads up to trust the classification given by KNN more so than that given by Discriminant Analysis. Hence, we are more confident in the assertion that Mary Shelley wrote Frankenstein than any other author given as a result of our Stylometric analysis and validation of the methodology used to arrive at this conclusion. 

```{r}
# thiis is just for working out the theta values given to see how they compare with one another for 1)William Godwin 2)Frankenstein 3)Mary SHelley. 
wp_thetas <- NULL
wp_books <- M$features[11]
wp_words <- apply(M$features[[11]], 2, sum)
inds <- which(wp_words==0) 
if (length(inds) > 0) {wp_words[inds] <- 1}
wp_thetas <- rbind(wp_thetas, wp_words/sum(wp_words))

wp_thetas

fr_thetas <- NULL
fr_book <- M$features[9]
fr_words <- apply(M$features[[9]], 2, sum)
Inds <- which(fr_words==0)
if (length(Inds)>0) {fr_words[Inds] <- 1}
fr_thetas <- rbind(fr_thetas, fr_words / sum(fr_words))

fr_thetas

ms_thetas <- NULL
ms_books <- M$features[4]
ms_words <- apply(M$features[[4]], 2, sum)
iinds <- which(ms_words == 0)
if (length(iinds)>0) {ms_words[iinds] <-1}
ms_thetas <- rbind(ms_thetas, ms_words / sum(ms_words))

rbind(fr_thetas, wp_thetas, ms_thetas)
dmultinom(x = fr_book[[1]], prob = wp_thetas, log=F)
```
Here we observe that there is disagreement between the models, with discriminant analysis predicting that William Godwin was the true author of Frankenstein, whilst the K-nearest-neighbours methodology predicts that Mary Shelley is the author of the novel. However from our cross-validation we have observed that KNN is far superior for identifying authorship on this dataset than discriminant analysis is. For example, viewing the confusion matrix above indicated that many books using discriminant analysis were misidentified as being the work of Mary Shelley, including one of Godwin's own books. K-nearest neighbours does not, however, have the same issues, demonstrating a high degree of accuracy when tested using cross-validation methods. 

Therefore, this leads up to trust the classification given by KNN more so than that given by Discriminant Analysis. 

```{r}
# This is just for testing some things
library(randomForest)
discriminantCorpus1 <- function(traindata, testdata) {
  thetas <- NULL
  
  # First, learn the model for each author
  for (i in 1:length(traindata)) {
    words <- apply(traindata[[i]], 2, sum)
    
    # Ensure no word has a zero count
    words[words == 0] <- 1
    thetas <- rbind(thetas, words / sum(words))
  }
  
  # Now classify
  preds <- matrix(0, nrow = nrow(testdata), ncol = nrow(thetas))
  for (i in 1:nrow(testdata)) {
    for (j in 1:nrow(thetas)) {
      preds[i, j] <- dmultinom(testdata[i, ], prob = thetas[j, ], log = TRUE)
    }
  }
  return(preds)
}

preds <- discriminantCorpus1(trainset, testset)
preds
# Compute probabilities
probs <- t(apply(preds, 1, function(log_likelihoods) {
  # Adjust log-likelihoods to prevent numerical underflow
  max_ll <- max(log_likelihoods)
  adjusted_lls <- log_likelihoods - max_ll
  likelihoods <- exp(adjusted_lls)
  # Normalize to get probabilities
  probabilities <- likelihoods / sum(likelihoods)
  return(probabilities)
}))
randomForestCorpus(trainset, testset)
probs #the other method in the package got it right tho???
```


