---
title: "SCS project1 draft"
output: html_notebook
---

```{r}
library(caret)

#change this to whatever your own directory is 
source("~/Desktop/SCSProj1/stylometryfunctions.R")

# as above
M <- loadCorpus("~/Desktop/SCSProj1/FunctionWords/", "frequentwords70")

set.seed(1)
```

### The Dataset

The data provided consisted of 'features' which are the word counts for the 70 function words used by each author. These are vectors of 70 elements for each individual book written by the authors. There is also author names which is used to connect the books with the known author of them. 

We used numerical identifiers as opposed to using the names of the authors to identify them with an index, making the process of prediction and training the model a lot more effective. In regards to the `features` list, each row represents an individual book and the columns are the individual function words themselves (columns are sort of immaterial in the sense that we don't often really deal with them explicitly, only in normalising lengths/statistics - SEE THE  FUNCTIONS FOR THAT).  



```{r}
# discriminant analysis and K-nearest neighbours

traindata <- M$features
testdata <- NULL
testlabels <- NULL

for( i in 1:length(traindata)){
  
  # keep author in training set if they have only the one book
  if (nrow(M$features[[i]]) <= 1){  
    next
  }
  
  #select a random book by this author by choosing a row (= book)
    testind <- sample(1:nrow(traindata[[i]]), 1)

    #add this book to the test set

    testdata <- rbind(testdata, traindata[[i]][testind,])

    testlabels <- c(testlabels, i)

    #now discard the book from the training set

    traindata[[i]] <- traindata[[i]][-testind,,drop=FALSE]
    
}

# obtaining DA prediction and accuracy (naive)
DApreds <- discriminantCorpus(traindata, testdata)
DAacc <- sum(DApreds == testlabels) / length(testlabels)

# obtaining KNN prediction and accuracy (naive)
KNNpreds <- KNNCorpus(traindata, testdata)
KNNacc <- sum(KNNpreds == testlabels) / length(testlabels)


```
---
---
```{r}


DApredictions <- NULL
KNNpredictions <- NULL
truth <- NULL
features <- M$features

# Filter out authors with only one document
authors_to_keep <- sapply(features, function(x) nrow(x) > 1)
features_filtered <- features[authors_to_keep]

for (i in 1:length(features_filtered)) {
  for (j in 1:nrow(features_filtered[[i]])) {
    testdata <- matrix(features_filtered[[i]][j, ], nrow = 1)
    traindata <- features_filtered
    traindata[[i]] <- traindata[[i]][-j, , drop = FALSE]
    
    pred <- discriminantCorpus(traindata, testdata)
    DApredictions <- c(DApredictions, pred)
    pred <- KNNCorpus(traindata, testdata)
    KNNpredictions <- c(KNNpredictions, pred)
    truth <- c(truth, i)
  }
}

DAacc<- sum(DApredictions == truth) / length(truth)
KNNacc <- sum(KNNpredictions == truth) / length(truth)
print("LOOCV accuracy for DA:")
DAacc
print("LOOCV accuracy for KNN:")
KNNacc


```
---


```{r}
library(caret)

DAconfusionmatrix <- confusionMatrix(as.factor(DApredictions), 
                                     as.factor(truth))

KNNconfusionmatrix <- confusionMatrix(as.factor(KNNpredictions),
                                      as.factor(truth))
print("DA cm:")
DAconfusionmatrix
print("---------------------------------------------")
print("KNN confusion matrix")
KNNconfusionmatrix
```



```{r}
# finding out who wrote frankenstein, using everything apart from 
# frankenstein in the training set and just that novel in the test set.

trainset <- M$features[-9]
testset <- M$features[9]
testset <- testset[[1]]
new_authors <- M$authornames[-9]

frankenstein_da <- discriminantCorpus(trainset, testset)
frankenstein_knn <- KNNCorpus(trainset, testset)

print("DA prediction of author:")
new_authors[frankenstein_da]
print("KNN prediction of author:")
new_authors[frankenstein_knn]


```
What this predicts is that Discriminant Analysis predicts that William Godwin wrote Frankenstein! Author 10 (+1=11, indexing issue.)


*** Probabilities were found in this gap, used to be a code chunk here, literally just ripping discriminantCorpus then using log-sum-exp trick in the line where which.max(probs) was to find the posterior probabilities. Ill probably put it back later if its genuinely becoming useful but doesnt look like it :(. ***


The discriminant corpus model appears to be overconfident that William Godwin wrote Frankenstein with its incredibly high probability (predicted as probability one). This could be for a variety of reasons. One is that there are a lot more (5) books in the total corpus written by this author, only Sir Walter Scott and Mary Shelley have more more books in the corpus (6). This could be indicative of there potentially not being valid assumptions to justify using such a model. The assumptions made are that some of the word counts are forced to not be zero. IT could also be a bias to authors who do not use a word very often, or did not in a certain book. Analysing Shelley's work further (V56) we see that in Frankenstein this word was used 3 times, and used it as many as 84 times in 'Lodore'. William Godwin, on the other hand, used V56 6 and 7 times in two novels, in Damon and Imogen respectively. This potential bias presented due to the extreme usage/disusage of specific words may pose a challenge to this particular kind of model due to the methods used to calculate probabilities and the multinomial nature of the 'bag of words' hypothesis used in its formulation. 